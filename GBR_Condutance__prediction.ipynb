{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**"
      ],
      "metadata": {
        "id": "OD10INsf657y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script provides a predictive Machine Learning Model to Estimate Conductance of Twisted Bilayer Graphene devices presented in the article: Layer-Resolved Quantum Transport in Twisted Bilayer Graphene: Counterflow and Machine Learning Predictions. The process integrates data normalization, clustering with a Self-Organizing Map (SOM), and regression with Gradient Boosting Regressors (GBR).\n",
        "\n",
        "The workflow ensures that any new input follows the same transformation pipeline as the training data, maintaining consistency in predictions. The key steps include downloading pre-trained models and scalers, clustering new input data based on learned patterns, selecting the appropriate GBR model for the identified cluster, and producing a final conductance prediction.\n",
        "\n",
        "By structuring the workflow in an automated and modular way, this approach significantly reduces computational costs compared to traditional simulations while preserving accuracy in conductance predictions."
      ],
      "metadata": {
        "id": "6lvw1_Fl5DO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Guide: Running the Conductance Prediction Pipeline"
      ],
      "metadata": {
        "id": "C2ZmhmNR-fz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå User Guide: Running the Conductance Prediction Pipeline\n",
        "\n",
        "This user guide provides a step-by-step explanation of how to execute the conductance prediction pipeline in this Colab notebook. The notebook is structured into multiple sections, ensuring a seamless process from dependency installation to final predictions.\n",
        "This version includes condutance for all angles present in the training dataset, allowing for a more comprehensive modeling of conductance behavior across different TBG configurations.\n",
        "\n",
        "## 1.  Installing Required Dependencies\n",
        "\n",
        "*   Before running the main code, all necessary Python libraries must be installed. This is done in Section 1, where essential packages such as MiniSom, numpy, scikit-learn, and pandas are installed.\n",
        "*   Ensure that all required packages are installed before running the pipeline.\n",
        "\n",
        "\n",
        "## 2. Downloading and Loading Pre-Trained Models\n",
        "\n",
        "\n",
        "*   In Section 2, the script downloads pre-trained models and scalers required for predictions. It verifies that all files are correctly retrieved before proceeding.\n",
        "*   The script checks whether each file is present and successfully loaded.\n",
        "If any file is missing or fails to load, an error message is displayed for debugging.\n",
        "\n",
        "## 3. Loading and Initializing the Models\n",
        "\n",
        "*   This step ensures that all necessary components are in memory before making predictions.\n",
        "\n",
        "## 4. Making Predictions with a New Input\n",
        "\n",
        "*   In section 4, users can input a new angle configuration to predict conductance. To perform a prediction, type the angle values in the variable.\n",
        "\n",
        "* The energy (ùê∏) will be in the range of 0.285 to 0.306. Values ‚Äã‚Äãoutside this range can lead to unreliable predictions as the model has not been trained beyond these limits.\n",
        "\n",
        "*   Angle (ùúÉ) (twist angle between graphene layers), must be within the range 1.17¬∞ to 4¬∞. Any value outside this range produce inaccurate results.  Values ‚Äã‚Äãoutside this range can lead to unreliable predictions as the model has not been trained beyond these limits.\n",
        "\n",
        "*   Arrival and Exit Pairs were indexed according to the following mapping:\n",
        "\n",
        "*   The \"u\" (up) and \"d\" (down) indicate the layer of the bilayer graphene where the contact is located.\n",
        "![TBG Device](https://drive.google.com/uc?export=view&id=1mg4eskPu94C0zm0Dwt5EioFTZxFmLDpX)\n"
      ],
      "metadata": {
        "id": "6K4y5E33-Lkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running the code**\n"
      ],
      "metadata": {
        "id": "B6i-g3b0j3mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Press the \"Play\" button in every code cell\n",
        "*   In section 3, user have to type the choosen angle to predict after running the cell. ( It shows up bellow the code cell)"
      ],
      "metadata": {
        "id": "nuZnoYHRkbTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Environment Setup and Package Installation**\n",
        "\n",
        "*  If an error message shows up during the installation process, you can safely ignore it. The warning is caused by version conflicts with other packages that are not required for this program.\n",
        "\n",
        "*   Ignore the red \"run\" button after running the code cell.\n",
        "\n",
        "*  Wait for the message \"‚úÖ Environment setup complete. Ready to proceed!\" to run the next cell code."
      ],
      "metadata": {
        "id": "QQbOrsPI7OsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8_ALmFwoj0_"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£ Desinstalar TODOS os pacotes\n",
        "!pip freeze | xargs pip uninstall -y 2>/dev/null\n",
        "\n",
        "# 2Ô∏è‚É£ Instalar apenas os pacotes necess√°rios\n",
        "!pip install numpy==2.0.0 joblib==1.4.2 scikit-learn==1.6.0 minisom==2.3.3 gdown pandas --no-warn-script-location > /dev/null 2>&1\n",
        "\n",
        "# 3Ô∏è‚É£ Exibir mensagem de conclus√£o\n",
        "print(\"‚úÖ Environment setup complete. Ready to proceed!\")\n",
        "\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "# 3Ô∏è‚É£ Reiniciar o ambiente para garantir que tudo funcione corretamente\n",
        "import os\n",
        "os._exit(00)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Downloading and Verifying the Required Files"
      ],
      "metadata": {
        "id": "ZE83VPTg2nUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Downloading and Verifying the Required Files\n",
        "*  Wait for the print message \"‚úÖ Environment setup complete. Ready to proceed!\" to run the cell code bellow\n",
        "\n",
        "*   Retrieves pre-trained models, scalers, and the trained SOM from Google Drive.\n",
        "*   Ensures all required files are downloaded and successfully loaded."
      ],
      "metadata": {
        "id": "d2Woj7SE_OH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from minisom import MiniSom\n",
        "\n",
        "\n",
        "# List of required files from Google Drive (direct download links)\n",
        "files = {\n",
        "    \"minisom_clusterizer.pkl\": \"https://drive.google.com/uc?id=1Go6sv941JqSWCQG5ph50shK_kiGRFdHj\",\n",
        "    \"scaler_X_standard.pkl\": \"https://drive.google.com/uc?id=1J1oTumvZBXhSdpw6HtCU7OFLqedOyQrI\",\n",
        "    \"scaler_X_minmax.pkl\": \"https://drive.google.com/uc?id=1Pbx5Ra858LvD7Crj_snJfJTMPfhsK37Y\",\n",
        "    \"scaler_y_standard.pkl\": \"https://drive.google.com/uc?id=1JxghZVBzDyB0pIED4SW0MhpdgtTq5kBI\",\n",
        "    \"scaler_y_minmax.pkl\": \"https://drive.google.com/uc?id=1Iw_VO4a1nqPaEiBCV-UsuwtM10VhWisc\",\n",
        "\n",
        "    # GBR Models for each cluster\n",
        "    \"gbr_model_cluster_0.pkl\": \"https://drive.google.com/uc?id=1Jf2y8mph0GKJJe6OxvDUlbyFJvVQvEKF\",\n",
        "    \"gbr_model_cluster_1.pkl\": \"https://drive.google.com/uc?id=1nu4DcI73dSbGH32-6_1XZv1Kki8R7ooP\",\n",
        "    \"gbr_model_cluster_2.pkl\": \"https://drive.google.com/uc?id=1c3owJkSuKrVPl1TeyA0G4E5rWbxGVkMA\",\n",
        "    \"gbr_model_cluster_3.pkl\": \"https://drive.google.com/uc?id=1cRE_A06sjSyU7Ftluz7Wzmx_w6QrKNNN\",\n",
        "    \"gbr_model_cluster_4.pkl\": \"https://drive.google.com/uc?id=1qFHJQI85s32WA-vS3lPwtKytJlsHr1eS\",\n",
        "    \"gbr_model_cluster_5.pkl\": \"https://drive.google.com/uc?id=1Zp70J-eATA0c50pBkVKOfjqC_xolOkAU\",\n",
        "    \"gbr_model_cluster_6.pkl\": \"https://drive.google.com/uc?id=19V2RD_dQhCWlpICm647HRoREZ3b9Ni9C\",\n",
        "    \"gbr_model_cluster_7.pkl\": \"https://drive.google.com/uc?id=1H-1DDKV5cdDAQc69gtDh1McSnsNY9Siq\",\n",
        "    \"gbr_model_cluster_8.pkl\": \"https://drive.google.com/uc?id=1AKXrzH3Q8kSxMp6l15zJfLaAxA_6zkkv\",\n",
        "}\n",
        "\n",
        "# Download all required files from Google Drive\n",
        "for filename, url in files.items():\n",
        "    gdown.download(url, filename, quiet=False)\n",
        "\n",
        "print(\"‚úÖ All files have been successfully downloaded!\")\n",
        "\n",
        "# List all downloaded files in the current directory\n",
        "downloaded_files = os.listdir()\n",
        "print(\"üìÇ Downloaded files:\")\n",
        "print(downloaded_files)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Function to verify file loading\n",
        "def verify_file_loading(file_name, variable_name):\n",
        "    if file_name in downloaded_files:\n",
        "        try:\n",
        "            obj = joblib.load(file_name)\n",
        "            print(f\"‚úÖ {file_name} successfully loaded into variable {variable_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading {file_name}: {e}\")\n",
        "    else:\n",
        "        print(f\"‚ùå File {file_name} NOT found!\")\n",
        "\n",
        "# Verify MiniSom\n",
        "verify_file_loading(\"minisom_clusterizer.pkl\", \"som\")\n",
        "\n",
        "# Verify Normalizers\n",
        "verify_file_loading(\"scaler_X_standard.pkl\", \"scaler_X_standard\")\n",
        "verify_file_loading(\"scaler_X_minmax.pkl\", \"scaler_X_minmax\")\n",
        "verify_file_loading(\"scaler_y_standard.pkl\", \"scaler_y_standard\")\n",
        "verify_file_loading(\"scaler_y_minmax.pkl\", \"scaler_y_minmax\")\n",
        "\n",
        "# Verify GBR Models\n",
        "for i in range(9):\n",
        "    verify_file_loading(f\"gbr_model_cluster_{i}.pkl\", f\"gbr_model_cluster_{i}\")\n",
        "\n",
        "print(\"\\n‚úÖ Verification completed! If any errors occur, check the Google Drive file IDs.\")\n"
      ],
      "metadata": {
        "id": "UO_PcQqVYB7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Loading Models and making Predictions"
      ],
      "metadata": {
        "id": "BXjfxIsd3gR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Models and Performing Predictions\n",
        "\n",
        "*   Loads the trained MiniSOM, scalers, and GBR models for each cluster.\n",
        "\n",
        "\n",
        "### Making a prediction\n",
        "\n",
        "*   Normalize the input data (energy, angle, contact pairs) using the pre-trained scalers.\n",
        "\n",
        "*   Identifies the corresponding cluster using the trained SOM and selects the appropriate GBR model for prediction.\n",
        "\n",
        "*   Predict the conductance using the chosen GBR model.\n",
        "\n",
        "*   Denormalize the predicted conductance to return a final real-world value.\n",
        "\n",
        "\n",
        "### *** After running the code cell type the angle configuration bellow the code cell.***"
      ],
      "metadata": {
        "id": "5WUfw5WA_kx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# üìå Carregar MiniSom treinado\n",
        "som = joblib.load(\"minisom_clusterizer.pkl\")\n",
        "\n",
        "# üìå Obter dimens√µes do SOM\n",
        "som_x, som_y = som._weights.shape[:2]\n",
        "\n",
        "# üìå Carregar escaladores\n",
        "scaler_X_standard = joblib.load(\"scaler_X_standard.pkl\")\n",
        "scaler_X_minmax = joblib.load(\"scaler_X_minmax.pkl\")\n",
        "scaler_y_standard = joblib.load(\"scaler_y_standard.pkl\")\n",
        "scaler_y_minmax = joblib.load(\"scaler_y_minmax.pkl\")\n",
        "\n",
        "# üìå Carregar modelos GBR para cada cluster\n",
        "models = {i: joblib.load(f\"gbr_model_cluster_{i}.pkl\") for i in range(9)}\n",
        "\n",
        "print(\"‚úÖ Modelos e escaladores carregados!\")\n",
        "\n",
        "# üìå Fun√ß√£o para gerar todos os pares de contato v√°lidos\n",
        "def generate_contact_pairs():\n",
        "    return [(i, j) for i in range(1, 9) for j in range(1, 9) if i != j]\n",
        "\n",
        "# üìå Fun√ß√£o para prever condut√¢ncia para um √¢ngulo espec√≠fico\n",
        "def predict_conductance_for_angle(angle):\n",
        "    print(\"üîÑ Gerando pares de contato...\")\n",
        "\n",
        "    # üìå Gerar todas as combina√ß√µes de ArrivalPair e ExitPair\n",
        "    contact_pairs = generate_contact_pairs()\n",
        "\n",
        "    # üìå Criar DataFrame com todas as combina√ß√µes poss√≠veis\n",
        "    data = []\n",
        "    for arrival, exit_ in contact_pairs:\n",
        "        # üìå Criar 206 valores de energia no intervalo [0.285, 0.306]\n",
        "        energy_values = np.linspace(0.28, 0.31, 300)\n",
        "        for energy in energy_values:\n",
        "            data.append([energy, angle, arrival, exit_])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"Energy\", \"Angle\", \"ArrivalPair\", \"ExitPair\"])\n",
        "\n",
        "    print(f\"\\n‚úÖ DataFrame criado com {len(df)} amostras!\")  # Deve conter 9476 amostras\n",
        "\n",
        "    # üìå Exibir 5 amostras antes da normaliza√ß√£o\n",
        "    print(\"\\nüìå 5 Random Samples Before Normalization:\")\n",
        "    print(df.sample(n=5, random_state=42))\n",
        "\n",
        "    # üìå Normalizar os dados\n",
        "    df_standardized = scaler_X_standard.transform(df)\n",
        "    df_normalized = scaler_X_minmax.transform(df_standardized)\n",
        "\n",
        "    df_normalized_df = pd.DataFrame(df_normalized, columns=[\"Energy\", \"Angle\", \"ArrivalPair\", \"ExitPair\"])\n",
        "    print(\"\\nüìå 5 Random Samples After Normalization:\")\n",
        "    print(df_normalized_df.sample(n=5, random_state=42))\n",
        "\n",
        "    # üìå Clusterizar os dados com o MiniSom treinado\n",
        "    clusters = [som.winner(sample) for sample in df_normalized]\n",
        "    cluster_labels = [x[0] * som_y + x[1] for x in clusters]\n",
        "    df[\"Cluster\"] = cluster_labels\n",
        "\n",
        "    print(\"\\nüìå 5 Random Samples After Clustering:\")\n",
        "    print(df.sample(n=5, random_state=42))\n",
        "\n",
        "    # üìå Fazer previs√µes usando o modelo correspondente a cada cluster\n",
        "    predicted_conductance = []\n",
        "    for i, sample in enumerate(df_normalized):\n",
        "        cluster = cluster_labels[i]\n",
        "        model = models.get(cluster)\n",
        "\n",
        "        if model is None:\n",
        "            predicted_conductance.append(None)\n",
        "        else:\n",
        "            # üìå Fazer a previs√£o com o modelo GBR correspondente ao cluster\n",
        "            pred_norm = model.predict(sample.reshape(1, -1))\n",
        "\n",
        "            # üìå Desnormalizar a previs√£o\n",
        "            pred_std = scaler_y_minmax.inverse_transform(pred_norm.reshape(-1, 1))\n",
        "            pred_real = scaler_y_standard.inverse_transform(pred_std)\n",
        "\n",
        "            predicted_conductance.append(pred_real[0][0])\n",
        "\n",
        "            if i < 5:  # Exibir apenas 5 previs√µes para n√£o poluir o console\n",
        "                print(f\"\\nüìå Sample {i+1}: Cluster {cluster}, GBR Model {cluster} utilizado.\")\n",
        "                print(f\"   Entrada para GBR: {sample}\")\n",
        "                print(f\"   Predi√ß√£o normalizada: {pred_norm[0]}\")\n",
        "                print(f\"   Predi√ß√£o desnormalizada: {pred_real[0][0]}\")\n",
        "\n",
        "    # üìå Adicionar previs√µes ao DataFrame\n",
        "    df[\"PredictedConductance\"] = predicted_conductance\n",
        "\n",
        "    print(\"\\nüìå DataFrame Final com Previs√µes:\")\n",
        "    print(df.head(10))  # Exibir as 10 primeiras linhas com previs√µes\n",
        "\n",
        "    return df  # Retorna o DataFrame final para inspe√ß√£o\n",
        "\n",
        "# üìå Perguntar ao usu√°rio qual √¢ngulo deseja usar\n",
        "while True:\n",
        "    try:\n",
        "        new_angle = float(input(\"\\nüìå Type an angle for prediction between 1.17 and 4.0: \"))\n",
        "        break\n",
        "    except ValueError:\n",
        "        print(\"‚ùå Entrada inv√°lida! Digite um n√∫mero v√°lido para o √¢ngulo.\")\n",
        "\n",
        "# üìå Executar previs√£o sem salvar resultados\n",
        "final_df = predict_conductance_for_angle(new_angle)\n",
        "\n",
        "print(\"\\n‚úÖ Processo conclu√≠do! O DataFrame com previs√µes foi gerado.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CobqvywcdNZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Saving the results"
      ],
      "metadata": {
        "id": "i2456q_54O2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving results files to calculate resistance\n",
        "\n",
        "*   Normalize the input data (energy, angle, contact pairs) using the pre-trained scalers.\n",
        "\n",
        "*   Identify the cluster of the input using the MiniSom model.\n",
        "\n",
        "*   Select the appropriate GBR model for the identified cluster.\n",
        "\n",
        "*   Predict the conductance using the chosen GBR model.\n",
        "\n",
        "*   Denormalize the predicted conductance to return a final real-world value.\n"
      ],
      "metadata": {
        "id": "8hJ4T7yrdFOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import sys\n",
        "\n",
        "# üìå Definir a pasta onde os arquivos ser√£o salvos\n",
        "save_directory = r\"C:\\Users\\PICHAU\\Desktop\\Condut√¢ncias preditas\"  # << ALTERE AQUI SE NECESS√ÅRIO >>\n",
        "\n",
        "# üìå Criar diret√≥rio se n√£o existir\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# üìå Definir o mapeamento original dos contatos\n",
        "arrival_mapping = {'1u': 1, '2u': 2, '3u': 3, '4u': 4, '1d': 5, '2d': 6, '3d': 7, '4d': 8}\n",
        "exit_mapping = arrival_mapping.copy()\n",
        "\n",
        "# üìå Criar mapeamento reverso dos contatos (index -> nome real)\n",
        "reverse_mapping = {v: k for k, v in arrival_mapping.items()}\n",
        "\n",
        "# üìå Iterar sobre cada par ArrivalPair/ExitPair e salvar arquivos individuais\n",
        "saved_files = []\n",
        "for (arrival, exit_), group_df in final_df.groupby([\"ArrivalPair\", \"ExitPair\"]):\n",
        "    # üìå Converter √≠ndice de volta para o nome real do contato\n",
        "    arrival_name = reverse_mapping[arrival]\n",
        "    exit_name = reverse_mapping[exit_]\n",
        "\n",
        "    # üìå Converter √¢ngulo para formato \"3p0\" (substituir \".\" por \"p\")\n",
        "    angle_str = str(new_angle).replace(\".\", \"p\")\n",
        "\n",
        "    # üìå Formatar nome do arquivo usando os nomes reais dos contatos\n",
        "    filename = f\"G{arrival_name}{exit_name}_AA50x50_{angle_str}_0T_th.dat\"\n",
        "    output_file = os.path.join(save_directory, filename)\n",
        "\n",
        "    # üìå Selecionar as colunas corretas antes de salvar (Energy e PredictedConductance)\n",
        "    df_to_save = group_df[[\"Energy\", \"PredictedConductance\"]]\n",
        "\n",
        "    # üìå Salvar DataFrame no arquivo correspondente SEM cabe√ßalho e √≠ndice\n",
        "    df_to_save.to_csv(output_file, sep=\" \", index=False, header=False)\n",
        "\n",
        "    print(f\"‚úÖ Arquivo salvo: {output_file}\")\n",
        "    saved_files.append(output_file)\n",
        "\n",
        "# üìå Se estiver no Google Colab, compactar os arquivos e baixar\n",
        "if \"google.colab\" in sys.modules:\n",
        "    print(\"\\nüì¶ Criando um arquivo ZIP para download...\")\n",
        "\n",
        "    zip_filename = \"/content/predicted_conductances.zip\"\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        for file in saved_files:\n",
        "            zipf.write(file, os.path.basename(file))  # Adiciona ao ZIP sem os diret√≥rios\n",
        "\n",
        "    print(f\"\\nüì¶ Arquivo ZIP gerado: {zip_filename}\")\n",
        "\n",
        "    # üìå Baixar automaticamente o arquivo ZIP no Colab\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    print(\"\\nüì• O download do arquivo compactado foi iniciado!\")\n",
        "\n",
        "print(\"\\n‚úÖ Processo conclu√≠do! Todos os arquivos foram gerados e salvos.\")\n"
      ],
      "metadata": {
        "id": "-W7db9ZQeqvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the Resistance Calculation Script\n",
        "\n",
        "To compute the resistance from the predicted conductance values, you need to use an Octave script that processes the extracted data files. This script reads the conductance matrices and applies the necessary calculations to determine resistance values.\n",
        "\n",
        "Follow the steps below to download the script and prepare it for execution:\n",
        "\n",
        "1.   Download the Octave script (matriz_resistencia_8c_matheus.m) from the GitHub repository.\n",
        "2.   Move the script to the same directory where the extracted conductance files (.dat) are stored.\n",
        "3.   Modify the script parameters to match your dataset (explained in the next section).\n",
        "\n",
        "Once these steps are completed, you will be ready to run the script and obtain resistance values.\n",
        "\n",
        "üëá Run the following code cell to download the script automatically:"
      ],
      "metadata": {
        "id": "MXWDgO0NTRhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from google.colab import files\n",
        "\n",
        "# Baixar o arquivo do GitHub\n",
        "octave_script_url = \"https://raw.githubusercontent.com/MatheusHGK/GBR_TBG/main/matriz_resistencia_8c_matheus.m\"\n",
        "octave_script_filename = \"matriz_resistencia_8c_matheus.m\"\n",
        "\n",
        "urllib.request.urlretrieve(octave_script_url, octave_script_filename)\n",
        "print(f\"‚úÖ Downloaded {octave_script_filename}\")\n",
        "\n",
        "# Disponibilizar para download no navegador\n",
        "files.download(octave_script_filename)\n"
      ],
      "metadata": {
        "id": "O-Mw3WgDTRvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once the script is downloaded, follow these steps:\n",
        "\n",
        "1. Placing the Octave Script in the Correct Folder: Locate the folder where the extracted conductance files are stored. These files should have names like:\n",
        "2. Move the **\"matriz_resistencia_8c_matheus.m\"** file to conductance files folder. The script must be in the same directory as the condutance predicted files.\n",
        "\n",
        "# Resistance calculation\n",
        "\n",
        "Before running the MatLab script, you need to modify two key sections to ensure it correctly processes the dataset.\n",
        "\n",
        "1Ô∏è‚É£ Adjusting Input Parameters\n",
        "Open the **\"matriz_resistencia_8c_matheus.m\"** file in a text editor and update the following values:\n",
        "\n",
        "*   ang: Set this to the correct dataset angle.\n",
        "*   I1, I2: Define the leads where the current enters (I1) and exits (I2),  \n",
        "    where I2 is assumed to have zero voltage.\n",
        "*   V1, V2: Specify the leads for voltage measurement.\n",
        "\n",
        "**These values should align with your specific simulation setup.**\n",
        "\n",
        "\n",
        "2Ô∏è‚É£ Updating File Names to Match the Correct Angle\n",
        "The script loads .dat files using fixed filenames that include the angle (4p0). If your dataset was generated with a different angle, update all occurrences of 4p0 to match the correct value.\n",
        "\n",
        "For example:\n",
        "\n",
        "If the dataset uses 3.5¬∞, rename files to G2u1u_AA50x50_3p5_0T_th0.dat.\n",
        "If the dataset uses 5.0¬∞, rename files to G2u1u_AA50x50_5p0_0T_th0.dat.\n",
        "\n",
        "*   Ensure that all filenames exactly match the files in your directory to avoid errors during execution.\n",
        "    \n"
      ],
      "metadata": {
        "id": "wzCdTv6WTapz"
      }
    }
  ]
}